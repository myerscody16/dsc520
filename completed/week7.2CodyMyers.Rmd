---
title: "Week7.2CodyMyers"
author: "Cody Myers"
date: "7/25/2020"
output: html_document
---

```{r}
library("caTools")
library("class")
library("caret")
setwd("C:/Users/myers/OneDrive/Desktop/dsc520")
classifier_df <- read.csv("data/binary-classifier-data.csv")
head(classifier_df)
summary(classifier_df)
```

```{r}
split<-sample.split(classifier_df, SplitRatio=0.8)
split
```

```{r}
train_df <- subset(classifier_df, split="TRUE")
test_df <- subset(classifier_df, split="FALSE")
logistic_model<-glm(label ~  x + y, data = train_df, family = "binomial")
summary(logistic_model)
```

```{r}
result <- predict(logistic_model, test_df, type="response")
result <- predict(logistic_model, train_df, type="response")
confusion_matrix <- table(Actual_Value=train_df$label, Predicted_Value= result >0.5)
confusion_matrix
```

```{r}
(confusion_matrix[[1,1]] + confusion_matrix[[2,2]])/sum(confusion_matrix)
```

By observing the result of the logistic regression, we see that this model is accurate 58% of the time.

```{r}
sqrt(nrow(train_df))
knn.38 <- knn(train=train_df, test=test_df, cl=train_df$label, k=38 )
knn.39 <- knn(train=train_df, test=test_df, cl=train_df$label, k=39 )
accuracy.38 <- 100 * sum(test_df$label == knn.38)/nrow(test_df)
accuracy.38
accuracy.39 <- 100 * sum(test_df$label == knn.39)/nrow(test_df)
accuracy.39
table(knn.38, test_df$label)
table(knn.39, test_df$label)
confusionMatrix(table(knn.39, test_df$label))
```

The KNN model is showing a 97% accuracy, this is significantly better than the 58% we saw on the logistic regression we saw previously.

The reasoning behinf the large disparity between the logistic regression and nearest neighbor model is due to the parametric properties that exist within the logistic regression. In the case of this dataset, we see that using the non-parametric model (nearest neighbor), we have a significantly higher level of accuracy. 

